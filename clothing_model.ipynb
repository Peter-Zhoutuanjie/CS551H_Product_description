{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f21cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install git+https://github.com/huggingface/datasets.git\n",
    "# !pip install rouge_score\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip install bert_score\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4ae8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments,T5Model\n",
    ")\n",
    "from transformers import T5Tokenizer\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7d6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BEAM_SIZE = 4\n",
    "DEVICE = \"cpu\"\n",
    "MODEL_NAME = \"t5-large\"\n",
    "DATASET_NAME = \"e2e_nlg\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 20\n",
    "SAVE_EVAL_STRATEGY = 'epoch'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f473d",
   "metadata": {},
   "source": [
    "loading the data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be7009a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Women_s_Black_Crew_Neck_Basic_Cotton_Tshirt _ Boohoo_UK_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15b5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1800 entries, 0 to 1801\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   name        1800 non-null   object\n",
      " 1   gender      1800 non-null   object\n",
      " 2   attributes  1279 non-null   object\n",
      " 3   colour      1800 non-null   object\n",
      " 4   price       1800 non-null   object\n",
      " 5   category    1800 non-null   object\n",
      " 6   desc        1800 non-null   object\n",
      "dtypes: object(7)\n",
      "memory usage: 112.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = data.dropna(subset=['desc'])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de785f",
   "metadata": {},
   "source": [
    "split training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "604ba269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['source'] = data.apply(\n",
    "    lambda x: '{} '.format(data['name']),\n",
    "    axis=1)\n",
    "data = data.drop(\n",
    "    ['name', 'gender', 'attributes', 'colour', 'price', 'category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913a38ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A classic wardrobe staple which no clothing co...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you’re going for a top-to-bottom wardrobe r...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hit refresh on your casual wardrobe with a ver...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A seriously comfy addition to your new-season ...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A menswear classic with a feminine edge, add a...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>Introducing your new fave top from our latest ...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>Introducing your new fave top from our latest ...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>Swapping out your jeans for something comfier?...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>Just chilling? Do it right with an oversized h...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>Introducing your new fave top from our latest ...</td>\n",
       "      <td>0                       High Waisted Disco Den...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   desc  \\\n",
       "0     A classic wardrobe staple which no clothing co...   \n",
       "1     If you’re going for a top-to-bottom wardrobe r...   \n",
       "2     Hit refresh on your casual wardrobe with a ver...   \n",
       "3     A seriously comfy addition to your new-season ...   \n",
       "4     A menswear classic with a feminine edge, add a...   \n",
       "...                                                 ...   \n",
       "1797  Introducing your new fave top from our latest ...   \n",
       "1798  Introducing your new fave top from our latest ...   \n",
       "1799  Swapping out your jeans for something comfier?...   \n",
       "1800  Just chilling? Do it right with an oversized h...   \n",
       "1801  Introducing your new fave top from our latest ...   \n",
       "\n",
       "                                                 source  \n",
       "0     0                       High Waisted Disco Den...  \n",
       "1     0                       High Waisted Disco Den...  \n",
       "2     0                       High Waisted Disco Den...  \n",
       "3     0                       High Waisted Disco Den...  \n",
       "4     0                       High Waisted Disco Den...  \n",
       "...                                                 ...  \n",
       "1797  0                       High Waisted Disco Den...  \n",
       "1798  0                       High Waisted Disco Den...  \n",
       "1799  0                       High Waisted Disco Den...  \n",
       "1800  0                       High Waisted Disco Den...  \n",
       "1801  0                       High Waisted Disco Den...  \n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14abdaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.iloc[:1700]   # First two rows of the dataframe\n",
    "df_test = data.iloc[1700:]   # Remaining rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7baf5bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656\n"
     ]
    }
   ],
   "source": [
    "cell_lengths = df_train['source'].str.len()\n",
    "\n",
    "# Get the length of the longest cell in the 'col1' column\n",
    "max_length = cell_lengths.max()\n",
    "\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19691011",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_train_test = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f933380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['desc', 'source', '__index_level_0__'],\n",
       "        num_rows: 1700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['desc', 'source', '__index_level_0__'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8c1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_for_batch(batch):\n",
    "    \"\"\"Construct input strings from a batch.\"\"\"\n",
    "    print(batch[\"source\"])\n",
    "    source = batch[\"source\"]\n",
    "    target = batch[\"desc\"]\n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f35126e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tokenize(batch, tokenizer, max_length=32):\n",
    "    \"\"\"Construct the batch (source, target) and run them through a tokenizer.\"\"\"\n",
    "    source, target = construct_input_for_batch(batch)\n",
    "    res = {\n",
    "        \"input_ids\": tokenizer(source)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(\n",
    "            target,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ),\n",
    "    }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f275a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhoutuanjie/opt/anaconda3/envs/torch/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774607cd283b47839f1ec7a19d792c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'list'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'torch.Tensor'>)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME)\n\u001b[0;32m----> 3\u001b[0m train_data_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets_train_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LENGTH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m valid_data_tokenized \u001b[38;5;241m=\u001b[39m datasets_train_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m batch: batch_tokenize(batch, tokenizer, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH),\n\u001b[1;32m      9\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:2989\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2982\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   2983\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2984\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2987\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2988\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 2989\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   2990\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   2991\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3365\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3361\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3362\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3363\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3365\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3369\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3370\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3374\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3257\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3255\u001b[0m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable))\n\u001b[0;32m-> 3257\u001b[0m     \u001b[43mvalidate_function_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m update_data:\n\u001b[1;32m   3259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Nothing to update, let's move on\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3223\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.validate_function_output\u001b[0;34m(processed_inputs, indices)\u001b[0m\n\u001b[1;32m   3219\u001b[0m all_dict_values_are_lists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m   3220\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, allowed_batch_return_types) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   3221\u001b[0m )\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_dict_values_are_lists \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m-> 3223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `function` which is applied to all elements of table returns a `dict` of types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mvalues()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. When using `batched=True`, make sure provided `function` returns a `dict` of types like `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_batch_return_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3225\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Provided `function` which is applied to all elements of table returns a `dict` of types [<class 'list'>, <class 'transformers.tokenization_utils_base.BatchEncoding'>]. When using `batched=True`, make sure provided `function` returns a `dict` of types like `(<class 'list'>, <class 'numpy.ndarray'>, <class 'pandas.core.series.Series'>, <class 'torch.Tensor'>)`."
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_data_tokenized = datasets_train_test['train'].map(\n",
    "    lambda batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n",
    "    batched=True\n",
    ")\n",
    "valid_data_tokenized = datasets_train_test['test'].map(\n",
    "    lambda batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528829da",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor_scorer = load_metric('meteor')\n",
    "\n",
    "def meteor_metric_builder(tokenizer):\n",
    "    def compute_meteor_metrics(pred):\n",
    "        \"\"\"Utility to compute meteor during training.\"\"\"\n",
    "        labels_ids = pred.label_ids\n",
    "        pred_ids = pred.predictions\n",
    "        # All special tokens are removed.\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "        label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        # Compute the metric.\n",
    "        meteor_results = meteor_scorer.compute(predictions=pred_str,\n",
    "                                       references=label_str)\n",
    "        return {\n",
    "            \"meteor\": round(meteor_results['meteor'], 4),\n",
    "        }\n",
    "    return compute_meteor_metrics\n",
    "\n",
    "meteor_metric_fn = meteor_metric_builder(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef9398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    DEVICE = torch.ones(1, device=mps_device)\n",
    "    print(DEVICE)\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = \"cuda:0\"\n",
    "    print(DEVICE)\n",
    "    \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c08e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"t5-v1_1-base-E2E\",\n",
    "    evaluation_strategy=SAVE_EVAL_STRATEGY,\n",
    "    save_strategy=SAVE_EVAL_STRATEGY,\n",
    "    logging_steps=5,\n",
    "    # optimization args, the trainer uses the Adam optimizer\n",
    "    # and has a linear warmup for the learning rate\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-03,\n",
    "    num_train_epochs=8,\n",
    "    warmup_steps=1000,\n",
    "    # misc args\n",
    "    seed=RANDOM_SEED,\n",
    "    disable_tqdm=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"meteor\",\n",
    "    # generation\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_data_tokenized,\n",
    "    eval_dataset=valid_data_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=meteor_metric_fn,\n",
    ")\n",
    "\n",
    "trainer._max_length = MAX_LENGTH\n",
    "trainer._num_beams = BEAM_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a503eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84283651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_generate_sentences(batch,\n",
    "                            model,\n",
    "                            tokenizer,\n",
    "                            num_beams=4,\n",
    "                            max_length=128,\n",
    "                            device='cuda:0'):\n",
    "    \"\"\"Generate outputs from a model with beam search decoding.\"\"\"\n",
    "    # Create batch inputs.\n",
    "    source, _ = construct_input_for_batch(batch)\n",
    "    # Use the model's tokenizer to create the batch input_ids.\n",
    "    batch_features = tokenizer(source, padding=True, return_tensors='pt')\n",
    "    # Move all inputs to the device.\n",
    "    batch_features = dict([(k, v.to(device))\n",
    "                           for k, v in batch_features.items()])\n",
    "\n",
    "    # Generate with beam search.\n",
    "    generated_ids = model.generate(\n",
    "        **batch_features,\n",
    "        num_beams=num_beams,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    # Use model tokenizer to decode to text.\n",
    "    generated_sentences = [\n",
    "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
    "        for gen_ids in generated_ids\n",
    "    ]\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_output = [X_test,y_test].map(\n",
    "    lambda batch: {\n",
    "        'generated':\n",
    "        beam_generate_sentences(batch,\n",
    "                                model,\n",
    "                                tokenizer,\n",
    "                                num_beams=BEAM_SIZE,\n",
    "                                max_length=MAX_LENGTH,\n",
    "                                device=DEVICE)\n",
    "    },\n",
    "    batched=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28976a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for ROUGE-2/L\n",
    "rouge_scorer = load_metric(\"rouge\")\n",
    "\n",
    "rouge_results = rouge_scorer.compute(\n",
    "    predictions=valid_output[\"generated\"],\n",
    "    references=valid_output[\"human_reference\"],\n",
    "    rouge_types=[\"rougeL\"],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=False,\n",
    ")\n",
    "rougeL = rouge_results['rougeL'].mid.fmeasure\n",
    "f\"R-L: {rouge_results['rougeL'].mid.fmeasure:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for meteor\n",
    "\n",
    "meteor_results = meteor_scorer.compute(predictions=valid_output[\"generated\"],\n",
    "                                       references=valid_output[\"human_reference\"])\n",
    "meteor = meteor_results['meteor']\n",
    "meteor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a245ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load_metric(\"bertscore\")\n",
    "bertscore_results = bertscore.compute(predictions=valid_output[\"generated\"],\n",
    "                                      references=valid_output[\"human_reference\"],\n",
    "                                      model_type='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f7cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(lst):\n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "bert_average_precision = average(bertscore_results['precision'])\n",
    "bert_average_recall = average(bertscore_results['recall'])\n",
    "bert_average_f1 = average(bertscore_results['f1'])\n",
    "\n",
    "f'average_precision: {bert_average_precision}, average_recall: {bert_average_recall},average_f1: {bert_average_f1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c310f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score_merics = [\n",
    "    \"rougeL\", \"meteor\", \"bert_average_precision\", \"bert_average_recall\",\n",
    "    \"bert_average_f1\"\n",
    "]\n",
    "bert_score_list = [\n",
    "    rougeL, meteor, bert_average_precision, bert_average_recall,\n",
    "    bert_average_f1\n",
    "]\n",
    "\n",
    "dataf = pd.DataFrame({\n",
    "    \"bert_score_merics\": bert_score_merics,\n",
    "    \"bert_score_list\": bert_score_list\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=80)\n",
    "sns.barplot(x=\"bert_score_merics\",\n",
    "            y=\"bert_score_list\",\n",
    "            data=dataf,\n",
    "            palette='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87809d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(valid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd94f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607200a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9562d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591b866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac23cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c5fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a05cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8550abf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f7872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d17bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588a168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f69e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffdaa3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a7d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23cf662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fed1f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4d35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5a39c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c264e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347d9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
